---
title: "Workflow management with Haskell Shake I: Discovery"
author: Clemens Schmid
origin: https://medium.com/@nevrome/my-workflow-automation-journey-discovering-shake-haskell-5c270b93ff2b
---

## Introduction

*This is part I of a two part blog post. See [part II](/posts/2021-12-05-shake-II.html) for a little showcase of Shake.*

Workflow management, so software to organize and run data analysis scripts, is one of these fortunate domains, where dozens of open source solutions are competing for our attention. There’s probably something for every taste (see e.g. the extensive list [here](https://github.com/pditommaso/awesome-pipeline)), and many of these projects are actively maintained or at least comparatively easy to resurrect. This post is an attempt to describe my personal journey for a tool that fits me, in the hope to motivate you to go searching as well.

## My user story

My PhD research is located somewhere between Bioinformatics and Archaeoinformatics (yep — that’s [a thing](https://caa-international.org/)) and I work with large and high-dimensional datasets. Not really Big data, but big enough to require a high performance computing environment to run analyses in reasonable time. Space, time and (ancient)DNA meet in my data, so my code necessarily relies on a variety of software libraries from different domains. In the last two years I piled scripts on top of scripts and thus created a complex network of interlinked code for data preparation, analysis and visualization.

This is my personal user story. It eventually brought me to a point where I realized that I have to introduce a more sophisticated system for dependency management and workflow automation. The former is especially important for reproducibility, and the latter to propagate changes, so to always maintain an up-to-date version of derived data products and plots. I needed a system that defines, runs and monitors a pipeline of code across different interacting scripts.

As I share these challenges with a large number of people working professionally with computers, there are many excellent solutions for exactly these challenges out there. I just had to pick what fits me, my tasks and my interests. So I decided to follow my gut feelings and ended up with the containerization solutions docker and singularity to encapsulate my development environment (which will only be mentioned in passing here), and the build system [**Shake**](https://shakebuild.com/) to orchestrate my analysis pipeline.

## Why Shake, of all things?

The first options l considered for pipeline management were [**Nextflow**](https://www.nextflow.io/) and [**Snakemake**](https://snakemake.readthedocs.io/en/stable/). Both are very popular among my colleagues in bioinformatics. At our department there seems to be an even divide between strong fans of the former and the latter. I personally did not want to deal neither with [Groovy](http://groovy-lang.org/documentation.html) nor with Python, though, which nextflow and snakemake respectively use as an underlying configuration language. Ideally I wanted to write the pipeline definition in a language and framework I’m already familiar with. That’s not (only) laziness. By working in either R or Haskell, with which I feel most comfortable, I could more easily leverage the power of these languages.

So then I gave some scrutiny to [**targets**](https://books.ropensci.org/targets/walkthrough.html), an implementation of a pipelining tool in R. This might have worked for me, but it gave me the impression to be too focused on workflows within R. R is certainly an important component of my personal tech stack right now, but I wanted to be prepared for whatever the future might bring. I also — and that’s very shallow— didn’t like target’s syntax from what I saw in the example code, where every computation in a pipeline got crammed into a single list object.

At this point I realized I would really like to solve this in Haskell, as the language became something of a personal passion anyway. A functional, strongly typed language should also — at least in theory — be a good fit to formalize building rules. I did some research and came across three Haskell tools that seem to offer workflow management: [**Funflow**](https://github.com/tweag/funflow), [**Porcupine**](https://github.com/tweag/porcupine) and [**Bioshake**](https://github.com/PapenfussLab/bioshake). Instead of diving into them one after the other, I took a step back and asked the excellent Haskell community on reddit for advice: [Experiences with workflow managers implemented in Haskell (funflow, porcupine, bioshake, ?)](https://old.reddit.com/r/haskell/comments/q0esys/experiences_with_workflow_managers_implemented_in/)

Fortunately [Justin Bedő](https://github.com/jbedo), the author of Bioshake, saw the post and gave me some insights about his implementation. At the time he had already moved one step further, and had discontinued the development of Bioshake for his new solution [**BioNix**](https://github.com/PapenfussLab/bionix), which solves both (!) dependency and worflow management with the fascinating [Nix](https://nixos.org/) infrastructure. As Nix is a big world on its own, I couldn’t follow him there. So I instead gave the Bioshake documentation a good read. And there I realized that Bioshake heavily relies on Shake internally: understanding Shake seemed to be inevitable to figuring out Bioshake. And Shake alone already turned out to be powerful and flexible enough for my current needs!

I had reached the end of my software exploration journey.

Your journey for a workflow management solution would certainly be different, and you would most likely reach different conclusions. But I encourage you to explore this realm, if you think you share a user story similar to mine. You can keep reading [here](/posts/2021-12-05-shake-II.html), if you want to see how I configured Shake to help me with my challenges.
